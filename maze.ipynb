 {
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**[Can Machine Learning Solve a Maze?](https://www.sciencebuddies.org/science-fair-projects/project-ideas/ArtificialIntelligence_p008/artificial-intelligence/machine-learning-maze)**\n",
        "\n",
        "This notebook was developed by Science Buddies [www.sciencebuddies.org](https://www.sciencebuddies.org/) as part of a science project to allow students to explore and learn about artificial intelligence. For personal use, this notebook can be downloaded and modified with attribution. For all other uses, please see our [Terms and Conditions of Fair Use](https://www.sciencebuddies.org/about/terms-and-conditions-of-fair-use).  \n",
        "\n",
        "**Troubleshooting tips**\n",
        "*   Read the written instructions at Science Buddies and the text and comments on this page carefully.\n",
        "*   If you make changes that break the code, you can download a fresh copy of this notebook and start over.\n",
        "\n",
        "*   If you are using this notebook for a science project and need help, visit our [Ask an Expert](https://www.sciencebuddies.org/science-fair-projects/ask-an-expert-intro) forum for assistance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i-yzpygV-KHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How To Use This Notebook**\n",
        "\n",
        "This notebook contains text fields, like this one, that give you information about the project and instructions."
      ],
      "metadata": {
        "id": "0wlxy2CwFibe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are also code blocks, like this one.\n",
        "\n",
        "# The green text in a code block are comments. Comments are descriptions of what the code does.\n",
        "\n",
        "# The non-green text in a code block is the Python code. Click on the triangle in the top left corner to run this code block.\n",
        "\n",
        "print(\"Congratulations, you ran a code block! Try changing the text in the code and running it again.\")"
      ],
      "metadata": {
        "id": "suVLY4Wj12KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing Libraries**\n",
        "We will start this science project by importing some necessary libraries. These libraries contain functions that we will be using to create and display our maze. The comments tell you what each libary is for."
      ],
      "metadata": {
        "id": "UBXJ9hup4A34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0g5ehcF-C35"
      },
      "outputs": [],
      "source": [
        "# Provides support for large, multi-dimensional arrays and matrices, as well as a wide range of mathematical functions\n",
        "# We will be using this library to initialize our maze.\n",
        "import numpy as np\n",
        "\n",
        "# A 2D plotting library that enables users to create a wide variety of high-quality plots and visualizations.\n",
        "# We will be using this library to display our maze in a visually appealing way.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# A library that provides various functions for working with time-related operations. We will be using this library\n",
        "# to give us time to look at how the agent is progressing through the maze\n",
        "import time\n",
        "\n",
        "print(\"You have imported all the libraries.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Creating the Maze Environment**"
      ],
      "metadata": {
        "id": "7OXdlVHe-O9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below sets up a simple maze with walls, a starting point (S), and a goal point (G). The maze is set up on a grid where each cell is either a 0 or 1, with 0 representing a black empty space and 1 representing a white wall. For your science project, you can use the sample maze we have provided, or change the code to make your own.\n",
        "\n",
        "If you make your own maze follow these guidelines:\n",
        "*   You can make it any size you want.\n",
        "*   Make sure that the start and goal positions are located inside the maze and not on a wall.\n",
        "*   Make sure your maze has a path from the start to the goal.\n"
      ],
      "metadata": {
        "id": "khPGglkq-R6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Maze:\n",
        "    def __init__(self, maze, start_position, goal_position):\n",
        "        # Initialize Maze object with the provided maze, start_position, and goal position\n",
        "        self.maze = maze\n",
        "        self.maze_height = maze_layout.shape[0] # Get the height of the maze (number of rows)\n",
        "        self.maze_width = maze_layout.shape[1]  # Get the width of the maze (number of columns)\n",
        "        self.start_position = start_position    # Set the start position in the maze as a tuple (x, y)\n",
        "        self.goal_position = goal_position      # Set the goal position in the maze as a tuple (x, y)\n",
        "\n",
        "    def show_maze(self):\n",
        "        # Visualize the maze using Matplotlib\n",
        "        plt.figure(figsize=(5,5))\n",
        "\n",
        "        # Display the maze as an image in grayscale ('gray' colormap)\n",
        "        plt.imshow(self.maze, cmap='gray')\n",
        "\n",
        "        # Add start and goal positions as 'S' and 'G'\n",
        "        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
        "        plt.text(self.goal_position[0], self.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
        "\n",
        "        # Remove ticks and labels from the axes\n",
        "        plt.xticks([]), plt.yticks([])\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "print(\"This code block has been run and the Maze class is now available for use.\")"
      ],
      "metadata": {
        "id": "BrfkCY7w-OY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create any maze layout you'd like, here's an example\n",
        "maze_layout = np.array([\n",
        "    [0, 1, 0, 0, 0],\n",
        "    [0, 1, 1, 1, 0],\n",
        "    [0, 0, 0, 1, 0],\n",
        "    [1, 1, 0, 1, 1],\n",
        "    [0, 0, 0, 0, 0]\n",
        "])\n",
        "\n",
        "# Create an instance of the maze and set the starting and ending positions\n",
        "maze = Maze(maze_layout, (0, 0), (4, 4))\n",
        "# Visualize the maze\n",
        "maze.show_maze()"
      ],
      "metadata": {
        "id": "fXssFI9q_gic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Implementing the Agent**"
      ],
      "metadata": {
        "id": "59R0EpWI_mQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is for the agent. The agent can move in four directions: up, down, left, and right. You do not need to change this code for your engineering project. As a variation to the project, you can (with a little bit of python knowledge) try making changes to the learning and exploration rate work as a variation to the project."
      ],
      "metadata": {
        "id": "tNX8sLyO_pIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions the agent can take: Up, Down, Left, Right. Each action is represented as a tuple of two values: (row_change, column_change)\n",
        "actions = [(-1, 0), # Up: Moving one step up, reducing the row index by 1\n",
        "          (1, 0),   # Down: Moving on step down, increasing the row index by 1\n",
        "          (0, -1),  # Left: Moving one step to the left, reducing the column index by 1\n",
        "          (0, 1)]   # Right: Moving one step to the right, increasing the column index by 1\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, maze, learning_rate=0.1, discount_factor=0.9, exploration_start=1.0, exploration_end=0.01, num_episodes=100):\n",
        "        # Initialize the Q-learning agent with a Q-table containing all zeros\n",
        "        # where the rows represent states, columns represent actions, and the third dimension is for each action (Up, Down, Left, Right)\n",
        "        self.q_table = np.zeros((maze.maze_height, maze.maze_width, 4)) # 4 actions: Up, Down, Left, Right\n",
        "        self.learning_rate = learning_rate          # Learning rate controls how much the agent updates its Q-values after each action\n",
        "        self.discount_factor = discount_factor      # Discount factor determines the importance of future rewards in the agent's decisions\n",
        "        self.exploration_start = exploration_start  # Exploration rate determines the likelihood of the agent taking a random action\n",
        "        self.exploration_end = exploration_end\n",
        "        self.num_episodes = num_episodes\n",
        "\n",
        "    def get_exploration_rate(self, current_episode):\n",
        "        # Calculate the current exploration rate using the given formula\n",
        "        exploration_rate = self.exploration_start * (self.exploration_end / self.exploration_start) ** (current_episode / self.num_episodes)\n",
        "        return exploration_rate\n",
        "\n",
        "    def get_action(self, state, current_episode): # State is tuple representing where agent is in maze (x, y)\n",
        "        exploration_rate = self.get_exploration_rate(current_episode)\n",
        "        # Select an action for the given state either randomly (exploration) or using the Q-table (exploitation)\n",
        "        if np.random.rand() < exploration_rate:\n",
        "            return np.random.randint(4) # Choose a random action (index 0 to 3, representing Up, Down, Left, Right)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state]) # Choose the action with the highest Q-value for the given state\n",
        "\n",
        "    def update_q_table(self, state, action, next_state, reward):\n",
        "        # Find the best next action by selecting the action that maximizes the Q-value for the next state\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "\n",
        "        # Get the current Q-value for the current state and action\n",
        "        current_q_value = self.q_table[state][action]\n",
        "\n",
        "        # Q-value update using Q-learning formula\n",
        "        new_q_value = current_q_value + self.learning_rate * (reward + self.discount_factor * self.q_table[next_state][best_next_action] - current_q_value)\n",
        "\n",
        "        # Update the Q-table with the new Q-value for the current state and action\n",
        "        self.q_table[state][action] = new_q_value\n",
        "\n",
        "print(\"This code block has been run and the QLearningAgent class is now available for use.\")"
      ],
      "metadata": {
        "id": "rPZ-3Uh3_oJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Defining the Reward System**"
      ],
      "metadata": {
        "id": "4FbuYR2CAn1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code determines the values for the reward system. The reward system provides feedback to the agent in reinforcement learning.\n",
        "\n",
        "Use these default settings to see how the agent performs when untrained, and when initially trained.  Then adjust the values and repeat the training and evalution steps outlines in the project idea.  Keep adjusting systematically until you have engineered the best possible reward system to have the AI learn to efficiently navigate a maze."
      ],
      "metadata": {
        "id": "vJcGuXGGAr9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "goal_reward = 100\n",
        "wall_penalty = -10\n",
        "step_penalty = -1\n",
        "\n",
        "print(\"The reward system has been defined.\")"
      ],
      "metadata": {
        "id": "ImjtvwVSAwTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing the Agent**"
      ],
      "metadata": {
        "id": "O5_ZzP1ZBB_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below tests how well the agent navigates the maze. It reports the total number of steps the agent took to get through the maze as well as the total reward the agent accumulated.\n",
        "\n",
        "Every time you want to test the agent, run the function ten times.  Each time note the number of steps the agent used to travel between the start and the goal, as well as the total reward. Calculate the average steps and average reward across all ten test trials.\n",
        "\n",
        "For your engineering project you will:\n",
        "1. Test the agent before training.\n",
        "2. Try solving the maze yourself by hand. What is the fewest number of steps you can take to solve the maze?\n",
        "2. Test the agent after training with the default reward system values.\n",
        "3. Make changes to the default reward system and test the agent.  You will repeat this until you have an agent who is learning very well."
      ],
      "metadata": {
        "id": "lnZqfOpMo22Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function simulates the agent's movements in the maze for a single episode.\n",
        "\n",
        "def finish_episode(agent, maze, current_episode, train=True):\n",
        "    # Initialize the agent's current state to the maze's start position\n",
        "    current_state = maze.start_position\n",
        "    is_done = False\n",
        "    episode_reward = 0\n",
        "    episode_step = 0\n",
        "    path = [current_state]\n",
        "\n",
        "    # Continue until the episode is done\n",
        "    while not is_done:\n",
        "        # Get the agent's action for the current state using its Q-table\n",
        "        action = agent.get_action(current_state, current_episode)\n",
        "\n",
        "        # Compute the next state based on the chosen action\n",
        "        next_state = (current_state[0] + actions[action][0], current_state[1] + actions[action][1])\n",
        "\n",
        "        # Check if the next state is out of bounds or hitting a wall\n",
        "        if next_state[0] < 0 or next_state[0] >= maze.maze_height or next_state[1] < 0 or next_state[1] >= maze.maze_width or maze.maze[next_state[1]][next_state[0]] == 1:\n",
        "            reward = wall_penalty\n",
        "            next_state = current_state\n",
        "        # Check if the agent reached the goal:\n",
        "        elif next_state == (maze.goal_position):\n",
        "            path.append(current_state)\n",
        "            reward = goal_reward\n",
        "            is_done = True\n",
        "        # The agent takes a step but hasn't reached the goal yet\n",
        "        else:\n",
        "            path.append(current_state)\n",
        "            reward = step_penalty\n",
        "\n",
        "        # Update the cumulative reward and step count for the episode\n",
        "        episode_reward += reward\n",
        "        episode_step += 1\n",
        "\n",
        "        # Update the agent's Q-table if training is enabled\n",
        "        if train == True:\n",
        "            agent.update_q_table(current_state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state for the next iteration\n",
        "        current_state = next_state\n",
        "\n",
        "    # Return the cumulative episode reward, total number of steps, and the agent's path during the simulation\n",
        "    return episode_reward, episode_step, path\n",
        "\n",
        "print(\"This code block has been run and the finish_episode function is now available for use.\")"
      ],
      "metadata": {
        "id": "uaHLrAd0ov3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function evaluates an agent's performance in the maze. The function simulates the agent's movements in the maze,\n",
        "# updating its state, accumulating the rewards, and determining the end of the episode when the agent reaches the goal position.\n",
        "# The agent's learned path is then printed along with the total number of steps taken and the total reward obtained during the\n",
        "# simulation. The function also visualizes the maze with the agent's path marked in blue for better visualization of the\n",
        "# agent's trajectory.\n",
        "\n",
        "def test_agent(agent, maze, num_episodes=1):\n",
        "    # Simulate the agent's behavior in the maze for the specified number of episodes\n",
        "    episode_reward, episode_step, path = finish_episode(agent, maze, num_episodes, train=False)\n",
        "\n",
        "    # Print the learned path of the agent\n",
        "    print(\"Learned Path:\")\n",
        "    for row, col in path:\n",
        "        print(f\"({row}, {col})-> \", end='')\n",
        "    print(\"Goal!\")\n",
        "\n",
        "    print(\"Number of steps:\", episode_step)\n",
        "    print(\"Total reward:\", episode_reward)\n",
        "\n",
        "    # Clear the existing plot if any\n",
        "    if plt.gcf().get_axes():\n",
        "        plt.cla()\n",
        "\n",
        "    # Visualize the maze using matplotlib\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(maze.maze, cmap='gray')\n",
        "\n",
        "    # Mark the start position (red 'S') and goal position (green 'G') in the maze\n",
        "    plt.text(maze.start_position[0], maze.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
        "    plt.text(maze.goal_position[0], maze.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
        "\n",
        "    # Mark the agent's path with blue '#' symbols\n",
        "    for position in path:\n",
        "        plt.text(position[0], position[1], \"#\", va='center', color='blue', fontsize=20)\n",
        "\n",
        "    # Remove axis ticks and grid lines for a cleaner visualization\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    plt.grid(color='black', linewidth=2)\n",
        "    plt.show()\n",
        "\n",
        "    return episode_step, episode_reward\n",
        "\n",
        "agent = QLearningAgent(maze)\n",
        "# Test the agent using the test_agent function\n",
        "test_agent(agent, maze)"
      ],
      "metadata": {
        "id": "k0iqqGUaAyto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you notice that the untrained agent takes *a lot* of steps? That is because the agent keeps backtracking and hitting walls. Before it has learned anything, it is blindly going through the maze, choosing its path randomly."
      ],
      "metadata": {
        "id": "ihUJlT3HNwgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setting Up the Reinforcement Learning Loop**"
      ],
      "metadata": {
        "id": "GGFiGxS9OORv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code for Q-learning, a basic reinforcement learning algorithm. This is used to train the agent. This code updates the Q-values based on the rewards it receives during exploration.  You do not need to change this code for your engineering project."
      ],
      "metadata": {
        "id": "m3Pw8SjLORia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(agent, maze, num_episodes=100):\n",
        "    # Lists to store the data for plotting\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "\n",
        "    # Loop over the specified number of episodes\n",
        "    for episode in range(num_episodes):\n",
        "        episode_reward, episode_step, path = finish_episode(agent, maze, episode, train=True)\n",
        "\n",
        "        # Store the episode's cumulative reward and the number of steps taken in their respective lists\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_steps.append(episode_step)\n",
        "\n",
        "    # Plotting the data after training is completed\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Cumulative Reward')\n",
        "    plt.title('Reward per Episode')\n",
        "\n",
        "    average_reward = sum(episode_rewards) / len(episode_rewards)\n",
        "    print(f\"The average reward is: {average_reward}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(episode_steps)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Steps Taken')\n",
        "    plt.ylim(0, 100)\n",
        "    plt.title('Steps per Episode')\n",
        "\n",
        "    average_steps = sum(episode_steps) / len(episode_steps)\n",
        "    print(f\"The average steps is: {average_steps}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"This code block has been run and the train_agent function is now available for use.\")"
      ],
      "metadata": {
        "id": "vdCGxQmOOfph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training the Agent**"
      ],
      "metadata": {
        "id": "_jrZKOEZOiKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the agent\n",
        "train_agent(agent, maze, num_episodes=100)"
      ],
      "metadata": {
        "id": "kdDChsZ5Ok2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the agent after training\n",
        "test_agent(agent, maze, num_episodes=100)"
      ],
      "metadata": {
        "id": "0t9kcynWO1Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the Agent**"
      ],
      "metadata": {
        "id": "WdMqOalyPyR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Test the agent again using the test_agent function. Run the code 10 times and note how many steps the agent takes to reach the goal. On average, how many steps does the agent take?\n",
        "2. Does the agent consistently find the shortest path from the starting position to the goal?"
      ],
      "metadata": {
        "id": "JHtm7byfP255"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experimenting and Improving**"
      ],
      "metadata": {
        "id": "gM0DhgMuP4sM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try experimenting with a different reward system to see how that affects how the agent learns. Copy or run this cell multiple times, changing the goal reward from 0 to 1, 10, 20, 30, etc.. and to 100, and 1000."
      ],
      "metadata": {
        "id": "5p8Zb4jTP7lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "goal_reward = 0\n",
        "wall_penalty = -10\n",
        "step_penalty = -1\n",
        "\n",
        "agent = QLearningAgent(maze)\n",
        "\n",
        "train_agent(agent, maze, num_episodes=100)"
      ],
      "metadata": {
        "id": "jHry-2UIP0iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Variation: Creating More Mazes**"
      ],
      "metadata": {
        "id": "JDlMoIRYQyJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new mazes and test how well the agent performs in more complex mazes with multiple paths and dead ends"
      ],
      "metadata": {
        "id": "iCz1tNtcQ1sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create any maze layout you'd like\n",
        "maze_layout = np.array([[]])\n",
        "\n",
        "# Create an instance of the maze and set the starting and ending positions\n",
        "start_x = #\n",
        "start_y = #\n",
        "goal_x = #\n",
        "goal_y = #\n",
        "maze = Maze(maze_layout, (start_x, start_y), (goal_x, goal_y))\n",
        "# Visualize the maze\n",
        "maze.show_maze()"
      ],
      "metadata": {
        "id": "z-mFNtlDQ7U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the agent object\n",
        "agent = QLearningAgent(maze)\n",
        "# Test the agent using the test_agent function\n",
        "test_agent(agent, maze)"
      ],
      "metadata": {
        "id": "Nq2YEtefRZjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the agent\n",
        "train_agent(agent, maze, num_episodes=100)"
      ],
      "metadata": {
        "id": "0_v2bxszRQO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing agent again after training, feel free to run multiple times\n",
        "test_agent(agent, maze)"
      ],
      "metadata": {
        "id": "-HGoLr8ARfBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
